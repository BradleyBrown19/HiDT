{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp adaptive_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb\n",
    "\n",
    "from HiDT.building_blocks import *\n",
    "from HiDT.data_bunch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Unet\n",
    "> \"we propose an architecture for image-to-image\n",
    "translation that combines the two well-known ideas: skip\n",
    "connections [22] and adaptive instance normalizations\n",
    "(AdaIN) [6]. We show that such a combination is feasible and leads to an architecture that preserves details much\n",
    "better than currently dominant AdaIN architectures without skip connections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"adaptiveunet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Encoder\n",
    "> \"In our experiments, the content encoder has two downsampling and four residual blocks; after each downsampling, only five channels are used for\n",
    "skip connections in order to limit the information flow\n",
    "through them\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, ni=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #downsampling\n",
    "        \n",
    "        down1 = Conv2dBlock(8, 16, ks=3, stride=2, norm=\"none\", activation=\"relu\")\n",
    "        down2 = Conv2dBlock(32, 64, ks=3, stride=2, norm=\"none\", activation=\"relu\")\n",
    "        \n",
    "        res_sizes = [(3,8),(16,16),(16,32),(64,128)]\n",
    "\n",
    "        #four residual blocks \n",
    "        res_blocks = [ResBlock(dim=d[0], dout=d[1]) for d in res_sizes]\n",
    "        \n",
    "        #final conv\n",
    "        final = Conv2dBlock(128, 128, ks=3, stride=1)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            res_blocks[0],\n",
    "            down1,\n",
    "            res_blocks[1],\n",
    "            res_blocks[2],\n",
    "            down2,\n",
    "            res_blocks[3],\n",
    "            final\n",
    "        )\n",
    "        \n",
    "        #hook activations for skip connections\n",
    "        def hook_fn(mod, inp, out):\n",
    "            self.hooks.append(out)\n",
    "        \n",
    "        for res in res_blocks:\n",
    "            res.register_forward_hook(hook_fn)\n",
    "        \n",
    "        #Hook outputs, don't need to hook last layer\n",
    "        self.hooks = []\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        self.hooks = []\n",
    "        xb = self.model(xb)\n",
    "        return xb, self.hooks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = ContentEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 64, 64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce(get_im())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_feat, hooks = ce(get_im())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Encoder\n",
    "> \"The style encoder contains four downsampling blocks. The output of the style encoder is a\n",
    "three-channel tensor, which is averaged-pooled into a three dimensional vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StyleEncoder(nn.Module):\n",
    "    def __init__(self, ni=3, num_down=4, latent_size=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.down_layers = [\n",
    "            Conv2dBlock(ni if i == 0 else 8*(2**(i-1)), 8*(2**i), ks=3, stride=2, norm=\"in\", activation=\"relu\") for i in range(num_down-1)\n",
    "        ] + [Conv2dBlock(8*(2**(num_down-2)), latent_size, ks=3, stride=2, norm=\"in\", activation=\"relu\")]\n",
    "        \n",
    "        self.downsampling = nn.Sequential(*self.down_layers)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        down = self.downsampling(xb)\n",
    "        return down.mean(dim=-1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = StyleEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_im().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds(get_im()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_feat = ds(get_im())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaIN Layer\n",
    "> \"AdaIN parameters are computed from the style vector via three-layer\n",
    "fully-connected network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_adastats(features):\n",
    "    bs, c = features.shape[:2]\n",
    "    features = features.view(bs, c, -1)\n",
    "    mean = features.mean(dim=2).view(bs,c,1,1)\n",
    "    std = features.var(dim=2).sqrt().view(bs,c,1,1)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AdaIN(content_feat, style_feat):\n",
    "    #calculating channel and batch specific stats\n",
    "    smean, sstd = get_adastats(style_feat)\n",
    "    cmean, cstd = get_adastats(content_feat)\n",
    "    \n",
    "    csize = content_feat.size()\n",
    "    \n",
    "    norm_content = (content_feat - cmean.expand(csize)) / cstd.expand(csize)\n",
    "    return norm_content * sstd.expand(csize) + smean.expand(csize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaSkipBlock(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.ada = AdaIN\n",
    "        self.dense = Conv2dBlock(nin*2, nin, ks=3, stride=1)\n",
    "        self.ada_creator = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.Linear(16,64),\n",
    "            nn.Linear(64,256),\n",
    "            Lambda(lambda x: x.view(x.shape[0], nin, -1))\n",
    "        )\n",
    "    \n",
    "    def forward(self, content, style, hook):\n",
    "        ada_params = self.ada_creator(style)\n",
    "        ada = self.ada(hook, ada_params)\n",
    "        combined = torch.cat([content, ada], dim=1)\n",
    "        return self.dense(combined)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaResBlock(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ada_block = AdaSkipBlock(nin, nin)\n",
    "        self.res_block = ResBlock(nin, nout)\n",
    "        \n",
    "        self.skip = Conv2dBlock(nin, nout, 3, 1, activation=\"none\") if nin != nout else None\n",
    "    \n",
    "    def forward(self, content, style, hook):\n",
    "        ada = self.ada_block(content, style, hook)\n",
    "        res = self.res_block(ada)\n",
    "        \n",
    "        if self.skip is not None:\n",
    "            content = self.skip(content)\n",
    "        \n",
    "        return res + content\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "> \"The decoder has five residual blocks\n",
    "with AdaIN layers and two upsampling blocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Upsampling blocks\n",
    "        self.up1 = UpBlock(64, 32)\n",
    "        self.up2 = UpBlock(16, 8)\n",
    "        \n",
    "        #AdaIN Res Blocks\n",
    "        ada_sizes = [(128, 128), (128, 64), (32, 16), (16,16), (8,3)]\n",
    "        \n",
    "        self.ada_skip_blocks = [\n",
    "            AdaResBlock(d[0], d[1]) for d in ada_sizes\n",
    "        ]\n",
    "       \n",
    "        \n",
    "    def forward(self, xb, style, hooks):\n",
    "        xb = self.ada_skip_blocks[0](xb, style, xb)\n",
    "        xb = self.ada_skip_blocks[1](xb, style, hooks[3])\n",
    "        xb = self.up1(xb)\n",
    "        xb = self.ada_skip_blocks[2](xb, style, hooks[2])\n",
    "        xb = self.ada_skip_blocks[3](xb, style, hooks[1])\n",
    "        xb = self.up2(xb)\n",
    "        xb = self.ada_skip_blocks[4](xb, style, hooks[0])\n",
    "        return xb\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(content_feat, style_feat, hooks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
