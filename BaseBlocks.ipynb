{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp building_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnetBlock(nn.Module):\n",
    "    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, hook:Hook, final_div:bool=True, blur:bool=False, leaky:float=None,\n",
    "                 self_attention:bool=False):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, leaky=leaky)\n",
    "        self.bn = batchnorm_2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = conv_layer(ni, nf, leaky=leaky)\n",
    "        self.conv2 = conv_layer(nf, nf, leaky=leaky, self_attention=self_attention)\n",
    "        self.relu = relu(leaky=leaky)\n",
    "\n",
    "    def forward(self, up_in:Tensor) -> Tensor:\n",
    "        s = self.hook.stored\n",
    "        up_out = self.shuf(up_in)\n",
    "        ssh = s.shape[-2:]\n",
    "        if ssh != up_out.shape[-2:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-2:], mode='nearest')\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_sfs_idxs(sizes:Sizes) -> List[int]:\n",
    "    \"Get the indexes of the layers where the size of the activation changes.\"\n",
    "    feature_szs = [size[-1] for size in sizes]\n",
    "    sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n",
    "    if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n",
    "    return sfs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpBlock(nn.Module):\n",
    "    \"Pixel shuffle upsampling for unet upstream\"\n",
    "    def __init__(self, ni, nf):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn = batchnorm_2d(nf)\n",
    "        self.conv = Conv2dBlock(nf, nf, ks=5, stride=1, norm=\"bn\", activation=\"relu\", padding=2)\n",
    "        self.shuf = PixelShuffle_ICNR(ni, nf, blur=False, leaky=None)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, xb, body=None):\n",
    "        up_out = self.shuf(xb)\n",
    "        \n",
    "        if(body is not None):\n",
    "            ssh = body.shape[-2:]\n",
    "            if ssh != up_out.shape[-2:]:\n",
    "                up_out = F.interpolate(up_out, body.shape[-2:], mode='nearest')\n",
    "            up_out = self.relu(up_out+self.bn(body))\n",
    "\n",
    "        xb = self.conv(up_out)\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UpResBlock(nn.Module):\n",
    "    \"Pixel shuffle upsampling for unet upstream\"\n",
    "    def __init__(self, ni, nf):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn = batchnorm_2d(nf)\n",
    "        self.conv = Conv2dBlock(nf, nf, ks=5, stride=1, norm=\"bn\", activation=\"relu\", padding=2)\n",
    "        self.shuf = PixelShuffle_ICNR(ni, nf, blur=False, leaky=None)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, xb, body=None):\n",
    "        og = xb\n",
    "        \n",
    "        up_out = self.shuf(xb)\n",
    "        \n",
    "        if(body is not None):\n",
    "            ssh = body.shape[-2:]\n",
    "            if ssh != up_out.shape[-2:]:\n",
    "                up_out = F.interpolate(up_out, body.shape[-2:], mode='nearest')\n",
    "            up_out = self.relu(up_out+self.bn(body))\n",
    "\n",
    "        xb = self.conv(up_out)\n",
    "        \n",
    "        return xb + F.interpolate(og, xb.shape[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=3, stride=2, norm=\"in\", activation=\"relu\", padding=1):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.pad = nn.ZeroPad2d(padding)\n",
    "        \n",
    "        norm_dim = nf\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'ln':\n",
    "            self.norm = LayerNorm(norm_dim)\n",
    "        elif norm == 'adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "            \n",
    "        self.conv = nn.Conv2d(ni, nf, ks, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        # print(x.size())\n",
    "        if x.size(0) == 1:\n",
    "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
    "            mean = x.view(-1).mean().view(*shape)\n",
    "            std = x.view(-1).std().view(*shape)\n",
    "        else:\n",
    "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dout, norm='in', activation='relu', ks=3, padding=1, auto=True):\n",
    "        super(ResBlock, self).__init__()\n",
    "        if auto: padding = ks // 2\n",
    "        self.model = []\n",
    "        self.model += [Conv2dBlock(dim, dim, ks, 1, norm, activation, padding)]\n",
    "        self.model += [Conv2dBlock(dim, dout, ks, 1, norm, activation, padding)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "        \n",
    "        self.skip = Conv2dBlock(dim, dout, 3, 1, norm, activation=\"none\", padding=padding) if dim != dout else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = self.model(x)\n",
    "        \n",
    "        if self.skip is not None:\n",
    "            x = self.skip(x)\n",
    "            \n",
    "        return res + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlocks(nn.Module):\n",
    "    def __init__(self, num_blocks, dim, norm='in', activation='relu', padding=1):\n",
    "        super(ResBlocks, self).__init__()\n",
    "        self.blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks += [ResBlock(dim, norm=norm, activation=activation, padding=padding)]\n",
    "        self.model = nn.Sequential(*self.blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_and_res(ni, nf): return nn.Sequential(res_block(ni), conv_layer(ni, nf, stride=2, bias=True, use_activ=False, leaky=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DisResBlock(nn.Module):\n",
    "    \"Resblock for projection discriminator, adapted from: https://github.com/XHChen0528/SNGAN_Projection_Pytorch\"\n",
    "    def __init__(self, in_ch, out_ch, h_ch=None, ksize=3, pad=1,\n",
    "                 activation=F.relu, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.learnable_sc = (in_ch != out_ch) or downsample\n",
    "        if h_ch is None:\n",
    "            h_ch = in_ch\n",
    "        else:\n",
    "            h_ch = out_ch\n",
    "\n",
    "        self.c1 = torch.nn.utils.spectral_norm(nn.Conv2d(in_ch, h_ch, ksize, 1, pad))\n",
    "        self.c2 = torch.nn.utils.spectral_norm(nn.Conv2d(h_ch, out_ch, ksize, 1, pad))\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = torch.nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.c1.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c2.weight.data, math.sqrt(2))\n",
    "        if self.learnable_sc:\n",
    "            init.xavier_uniform_(self.c_sc.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.residual(x)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        if self.downsample:\n",
    "            return F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "\n",
    "    def residual(self, x):\n",
    "        h = self.c1(self.activation(x))\n",
    "        h = self.c2(self.activation(h))\n",
    "        if self.downsample:\n",
    "            h = F.avg_pool2d(h, 2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimizedBlock(nn.Module):\n",
    "    \"Projection based discrminator, adapted from: https://github.com/XHChen0528/SNGAN_Projection_Pytorch\"\n",
    "    def __init__(self, in_ch, out_ch, ksize=3, pad=1, activation=F.relu):\n",
    "        super(OptimizedBlock, self).__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "        self.c1 = torch.nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, ksize, 1, pad))\n",
    "        self.c2 = torch.nn.utils.spectral_norm(nn.Conv2d(out_ch, out_ch, ksize, 1, pad))\n",
    "        self.c_sc = torch.nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 1, 1, 0))\n",
    "\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        init.xavier_uniform_(self.c1.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c2.weight.data, math.sqrt(2))\n",
    "        init.xavier_uniform_(self.c_sc.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shortcut(x) + self.residual(x)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        return self.c_sc(F.avg_pool2d(x, 2))\n",
    "\n",
    "    def residual(self, x):\n",
    "        h = self.activation(self.c1(x))\n",
    "        return F.avg_pool2d(self.c2(h), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
