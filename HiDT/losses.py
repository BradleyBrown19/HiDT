# AUTOGENERATED! DO NOT EDIT! File to edit: Losses.ipynb (unless otherwise specified).

__all__ = ['mse', 'cov', 'one_norm', 'get_adv_loss', 'get_rec_loss', 'get_latent_loss', 'get_sdist_loss', 'HiDTLoss']

# Cell
#export
from fastai import *
from fastai.vision import *
from fastai.callbacks import *
from fastai.utils.mem import *
from fastai.vision.gan import *
from PIL import Image

import numpy as np

import torch
import torch.nn.functional as F
import torch.nn as nn

from torch.utils.data import DataLoader
from torch.utils.data.dataset import TensorDataset
import pdb


# Cell
def mse(val, targ):
        return 0.5 * torch.mean((val-targ)**2)

# Cell
def cov(m):
    fact = 1.0 / (m.size(1) - 1)
    m -= torch.mean(m, dim=1, keepdim=True)
    mt = m.t()
    return fact * m.matmul(mt).squeeze()

# Cell
def one_norm(p1, p2):
    return (p1-p2).abs().sum()

# Cell
def get_adv_loss(preds):
    loss = 0
    for pred in preds:
        loss += mse(pred, torch.ones(pred.shape))
    return loss

# Cell
def get_rec_loss(orig, orig2, orig_recon, orig2_recon, cycled_orig, \
                cycled_orig2, one_rand, two_rand, one_rand_recon, two_rand_recon):
    return one_norm(orig, orig_recon) + one_norm(orig2, orig2_recon) + one_norm(orig, cycled_orig) \
                + one_norm(orig, cycled_orig2) + one_norm(one_rand, one_rand_recon) \
                + one_norm(two_rand, two_rand_recon)

# Cell
def get_latent_loss(l1, l2, l1rec, l2rec):
    return one_norm(l1, l1rec) + one_norm(l2, l2rec)

# Cell
def get_sdist_loss(orig_style, orig2_style, styles):
    styles = torch.cat([orig_style, orig2_style, *styles])

    smeans = styles.mean()
    cov_m = cov(styles)
    cov_diag = torch.diag(cov_m)

    return one_norm(smeans, torch.ones(1)) + one_norm(cov_m, torch.eye(cov_m.shape[0])) \
            + one_norm(cov_diag, torch.ones(cov_diag.shape))

# Cell
class HiDTLoss(GANModule):
    "Loss function wrapper"
    def __init__(self, loss_funcG:Callable, loss_funcC:Callable, model):
        super().__init__()
        self.loss_funcG,self.loss_funcC,self.model = loss_funcG,loss_funcC,model
        self.content_encoder, self.style_encoder, self.decoder, self.disc = model.content_encoder, model.style_encoder, model.decoder, model.discriminator
        self.style_cap = 16
        self.recent_styles = []

        self.lambda1, self.lambda2, self.lambda4, self.lambda5, self.lambda6, self.lambda7 = 5, 2, 1, 0.1, 4, 7

    def forward(self, *args):
        if self.gen_mode:
            return self.generator(*args)
        else:
            return self.discriminator(*args)

    def generator(self, output, *args):
        orig, orig2, orig_recon, orig2_recon, orig_style, orig2_style, orig_cont, orig2_cont, \
        one2two, two2one, one2two_style, two2one_style, one2two_cont, two2one_cont, cycled_orig, \
        cycled_orig2, one_rand, two_rand, one_rand_cont, two_rand_cont, one_rand_style, \
        two_rand_style, one_rand_recon, two_rand_recon, rand_style = output

        #adversarial losses
        adv_loss = get_adv_loss([one2two, two2one, one_rand, two_rand])

        #recreation losses
        rec_loss = get_rec_loss(orig, orig2, orig_recon, orig2_recon, cycled_orig, \
                                cycled_orig2, one_rand, two_rand, one_rand_recon, two_rand_recon)

        #TODO: add segmentation to training
        #seg_loss =

        #latent code losses
        lat_style_loss = get_latent_loss(orig_style, orig2_style, one2two_style, two2one_style)
        lat_style_rand_loss =  get_latent_loss(rand_style, rand_style, one_rand_style, two_rand_style)
        lat_cont_loss = get_latent_loss(orig_cont, orig2_cont, one2two_cont, two2one_cont)
        lat_cont_loss += get_latent_loss(orig_cont, orig2_cont, one_rand_cont, two_rand_cont)

        #style distribution loss
        sdist_loss = get_sdist_loss(orig_style, orig2_style, self.recent_styles)

        self.recent_styles.extend([orig_style.data, orig2_style.data])

        if len(self.recent_styles) >= self.style_cap:
            self.recent_styles = self.recent_styles[2:]

        return self.lambda1*adv_loss + self.lambda2*rec_loss + self.lambda4*lat_cont_loss \
                    + self.lambda5*lat_style_loss + self.lambda6*lat_style_rand_loss \
                    + self.lambda7 * sdist_loss

    def discriminator(self, output, *args):
        orig, orig2, one2two, two2one = output

        return mse(orig, torch.ones(orig.shape)) + mse(orig2, torch.ones(orig2.shape)) \
                + mse(one2two, torch.zeros(one2two.shape)) + mse(orig, torch.zeros(two2one.shape))
